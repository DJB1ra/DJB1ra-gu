{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f0fea34",
   "metadata": {
    "id": "5f0fea34",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdea1a9-d983-4e1c-9b21-6f24d93c9ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56578f6f",
   "metadata": {
    "id": "56578f6f"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761f4d2",
   "metadata": {
    "id": "1761f4d2"
   },
   "source": [
    "Loading the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aacba01e",
   "metadata": {
    "id": "aacba01e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You may need to edit the path, depending on where you put the files.\n",
    "data = pd.read_csv('a4_synthetic.csv')\n",
    "\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166d6ba",
   "metadata": {
    "id": "b166d6ba"
   },
   "source": [
    "Training a linear regression model for this synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01af42ba-6802-455c-bf07-4e7f0e53cc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.39164808632788506\n",
      "Epoch 2: MSE = 0.016984872842749174\n",
      "Epoch 3: MSE = 0.009564297806691957\n",
      "Epoch 4: MSE = 0.009377840166444317\n",
      "Epoch 5: MSE = 0.009368557944496174\n",
      "Epoch 6: MSE = 0.009367457445536662\n",
      "Epoch 7: MSE = 0.009367280149523092\n",
      "Epoch 8: MSE = 0.00936725047479511\n",
      "Epoch 9: MSE = 0.009367245535839945\n",
      "Epoch 10: MSE = 0.009367244720749514\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "np.random.seed(69)\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# Declare parameter tensors\n",
    "w = torch.tensor(w_init, requires_grad=True)\n",
    "b = torch.tensor(b_init, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = torch.optim.SGD([w, b], lr=eta)  # Define SGD optimizer\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    sum_err = 0\n",
    "    for row in range(X.shape[0]):\n",
    "        x = torch.tensor(X[[row], :],)\n",
    "        y = torch.tensor(Y[[row]])\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x @ w + b  # Matrix multiplication for prediction\n",
    "        err = torch.square((y_pred - y))  # Compute squared error loss\n",
    "\n",
    "        # Backward pass and update\n",
    "        \n",
    "        err.backward()  # Compute gradients\n",
    "        opt.step()  # Update parameters\n",
    "        opt.zero_grad()  # Clear gradients\n",
    "\n",
    "        # For statistics\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f221d",
   "metadata": {
    "id": "ee3f221d",
    "tags": []
   },
   "source": [
    "# Task 2, 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4057e2ea-0267-4d32-a610-e90233a41e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        if self.grad_fn is not None:\n",
    "            self.grad_fn.backward(grad_output)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(type(self))\n",
    "\n",
    "\n",
    "class AdditionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output)\n",
    "        self.right.backward(grad_output)\n",
    "        \n",
    "        \n",
    "class SubtractionNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output)\n",
    "        self.right.backward(grad_output)\n",
    "    \n",
    "        \n",
    "class MatrixMultiplicationNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output @ self.right.data.T)\n",
    "        self.right.backward(self.left.data.T @ grad_output)\n",
    "        \n",
    "        \n",
    "class ExponentiationNode(Node):\n",
    "    def __init__(self, tensor, exponent):\n",
    "        self.tensor = tensor\n",
    "        self.exponent = exponent\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        grad_input = self.exponent * self.tensor.data ** (self.exponent - 1) * grad_output\n",
    "        self.tensor.backward(grad_input)\n",
    "        \n",
    "        \n",
    "class TanhNode(Node):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (1 - np.tanh(self.tensor.data))**2\n",
    "        self.tensor.backward(grad_input)\n",
    "        \n",
    "\n",
    "class BinaryCrossEntropyLossNode(Node):\n",
    "    def __init__(self, prediction, target):\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        sig = 1 / (1 + np.exp(-self.prediction.data))  # Apply sigmoid function to prediction data\n",
    "        sig_neg_x = 1 / (1 + np.exp(self.prediction.data))\n",
    "    \n",
    "        # Calculate gradient of loss with respect to prediction\n",
    "        grad_loss = (-self.target.data * sig_neg_x + (1 - self.target.data) * sig) * grad_output\n",
    "        \n",
    "        # Backward pass to propagate gradient\n",
    "        self.prediction.backward(grad_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd7df64-4f09-486f-9860-32cbba605f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "\n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "\n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "\n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()\n",
    "\n",
    "    # For Task 2:\n",
    "\n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        # Call the helper function defined below.\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        return subtraction(self, right)\n",
    "\n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        return matmul(self, right)\n",
    "\n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return exponentiation(self, right)\n",
    "\n",
    "\n",
    "    def backward(self, grad_output=None):\n",
    "        if self.grad_fn is not None:\n",
    "            # If grad_fn is defined, we have computed this tensor using some operation.\n",
    "            if grad_output is None:\n",
    "                #raise an error if gradient of the loss function is required\n",
    "                self.grad_fn.backward(np.ones(self.shape))\n",
    "            else:\n",
    "                # This is an intermediate node in the computational graph.\n",
    "                self.grad_fn.backward(grad_output)\n",
    "        else:\n",
    "            # If grad_fn is not defined, this is an endpoint in the computational\n",
    "            # graph: learnable model parameters or input data.\n",
    "            if self.requires_grad:\n",
    "                self.grad = grad_output\n",
    "            else:\n",
    "                # This tensor *does not require* a gradient to be computed. This\n",
    "                # will typically be a tensor holding input data.\n",
    "                return\n",
    "            \n",
    "            # Tanh activation function\n",
    "    def tanh(self):\n",
    "        new_data = np.tanh(self.data)\n",
    "        grad_fn = TanhNode(self)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "\n",
    "# A small utility where we simply create a Tensor object. We use this to\n",
    "# mimic torch.tensor.\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "\n",
    "# We define helper functions to implement the various arithmetic operations.\n",
    "\n",
    "# This function takes two tensors as input, and returns a new tensor holding\n",
    "# the result of an element-wise addition on the two input tensors.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data\n",
    "    grad_fn = AdditionNode(left, right)\n",
    "    \n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def subtraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    grad_fn = SubtractionNode(left, right)\n",
    "    \n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def exponentiation(left, right):\n",
    "    # Check if the exponent is an integer and >= 2\n",
    "    if not isinstance(right, int):\n",
    "        raise ValueError(\"Exponent must be an integer\")\n",
    "    if right < 2:\n",
    "        raise ValueError(\"Exponent must be >= 2\")\n",
    "    new_data = left.data ** right\n",
    "    grad_fn = ExponentiationNode(left, right)\n",
    "    \n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def matmul(left, right):\n",
    "    # Check if the shapes of the tensors are compatible for matrix multiplication\n",
    "    if left.shape[1] != right.shape[0]:\n",
    "        raise ValueError(\"Shapes are not compatible for matrix multiplication\")\n",
    "    new_data = left.data @ right.data\n",
    "    grad_fn = MatrixMultiplicationNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0f04c",
   "metadata": {
    "id": "36d0f04c"
   },
   "source": [
    "Some sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2014827",
   "metadata": {
    "id": "f2014827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2 ** 2\n",
    "test_matmul = x1 @ w\n",
    "\n",
    "print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n",
    "print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n",
    "print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n",
    "print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n",
    "assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n",
    "assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n",
    "assert(np.allclose(test_matmul.data, np.array([[1.6]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1bb77-e869-4e08-8996-3674eed101e6",
   "metadata": {
    "id": "9cc1bb77-e869-4e08-8996-3674eed101e6"
   },
   "source": [
    "Sanity check for Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3276aba-4def-421b-b12e-bf0d7120f19e",
   "metadata": {
    "id": "f3276aba-4def-421b-b12e-bf0d7120f19e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: <class '__main__.AdditionNode'>\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AdditionNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a9bfb-ea55-4bce-9356-4956316e1904",
   "metadata": {
    "id": "529a9bfb-ea55-4bce-9356-4956316e1904"
   },
   "source": [
    "Sanity check for Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32687661-a67d-4bef-9a90-7dabb93380a2",
   "metadata": {
    "id": "32687661-a67d-4bef-9a90-7dabb93380a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w\n",
    "diff = model_out - y\n",
    "loss = diff ** 2\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient of loss w.r.t. w =\\n', w.grad)\n",
    "\n",
    "assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n",
    "assert(x.grad is None)\n",
    "assert(y.grad is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ed8e2",
   "metadata": {
    "id": "105ed8e2"
   },
   "source": [
    "An equivalent cell using PyTorch code. Your implementation should give the same result for `w.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e72a5687",
   "metadata": {
    "id": "e72a5687"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6000],\n",
       "        [8.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n",
    "pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "pt_y = torch.tensor(np.array([[0.2]]))\n",
    "\n",
    "pt_model_out = pt_x @ pt_w\n",
    "pt_model_out.retain_grad()\n",
    "\n",
    "pt_diff = pt_model_out - pt_y\n",
    "pt_diff.retain_grad()\n",
    "\n",
    "pt_loss = pt_diff ** 2\n",
    "pt_loss.retain_grad()\n",
    "\n",
    "pt_loss.backward()\n",
    "pt_w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5439b",
   "metadata": {
    "id": "d0b5439b"
   },
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b03a8c5",
   "metadata": {
    "id": "0b03a8c5"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        # This method does nothing in the base optimizer class.\n",
    "        pass\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "\n",
    "            # Update parameter values based on the gradient and learning rate\n",
    "            param.data -= self.lr * param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aaf6728-41a8-448d-aab1-6a6131783d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.3916480863278851\n",
      "Epoch 2: MSE = 0.016984872842749174\n",
      "Epoch 3: MSE = 0.009564297806691956\n",
      "Epoch 4: MSE = 0.009377840166444315\n",
      "Epoch 5: MSE = 0.009368557944496175\n",
      "Epoch 6: MSE = 0.009367457445536664\n",
      "Epoch 7: MSE = 0.009367280149523092\n",
      "Epoch 8: MSE = 0.009367250474795108\n",
      "Epoch 9: MSE = 0.009367245535839947\n",
      "Epoch 10: MSE = 0.00936724472074951\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "np.random.seed(69)\n",
    "w_init = np.random.normal(size=(2, 1))\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# Declare parameter tensors\n",
    "w = tensor(w_init, requires_grad=True)\n",
    "b = tensor(b_init, requires_grad=True)\n",
    "\n",
    "eta = 1e-2\n",
    "opt = SGD([w, b], lr=eta)  # Use your own SGD optimizer\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):\n",
    "    sum_err = 0\n",
    "    for row in range(X.shape[0]):\n",
    "        x = tensor(X[[row], :])\n",
    "        y = tensor(Y[[row]])\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = x @ w + b  # Matrix multiplication for prediction\n",
    "        err = (y_pred - y) ** 2  # Compute squared error loss\n",
    "\n",
    "        # Backward pass and update\n",
    "        err.backward()  # Compute gradients\n",
    "        opt.step()  # Update parameters\n",
    "        opt.zero_grad()  # Clear gradients\n",
    "\n",
    "        # For statistics\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bef171",
   "metadata": {
    "id": "28bef171"
   },
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da62980a",
   "metadata": {
    "id": "da62980a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You may need to edit the path, depending on where you put the files.\n",
    "a4data = pd.read_csv('raisins.csv')\n",
    "\n",
    "X = scale(a4data.drop(columns='Class'))\n",
    "Y = 1.0*(a4data.Class == 'Besni').to_numpy()\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(Y))\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727929a2",
   "metadata": {
    "id": "727929a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((720, 7), (720,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1cd72d8-81d4-4452-a733-957bdd71aecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.24935912, -0.30801062, -0.00223273, ..., -0.26753351,\n",
       "         0.06541223, -0.25074774],\n",
       "       [-0.49779143, -0.58138353, -0.09820911, ..., -0.44678758,\n",
       "         0.49097416, -0.31407094],\n",
       "       [-0.18899551,  0.15973552, -0.44359745, ..., -0.16507106,\n",
       "        -0.47292075,  0.03800403],\n",
       "       ...,\n",
       "       [-0.69660911, -0.91892504, -0.24538342, ..., -0.70204792,\n",
       "         0.41234054, -0.82021434],\n",
       "       [ 0.65057947,  0.48147022,  0.85310643, ...,  0.62201236,\n",
       "         0.74952635,  0.53793425],\n",
       "       [ 0.24907038,  0.48264127,  0.05568096, ...,  0.22112035,\n",
       "        -0.19833912,  0.39097442]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03687ebd-f6e6-422f-844b-4d8fe043c75a",
   "metadata": {},
   "source": [
    "Class for the NN so we cna split up functions easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc54137-7802-4894-8dad-e6c1f9ad51bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr=0.01):\n",
    "        # Initialize weights and biases, inout layer need to match input to next layer etc.\n",
    "        self.w1 = tensor(np.random.normal(size=(input_size, hidden_size)), requires_grad=True)\n",
    "        self.b1 = tensor(np.random.normal(size=(1, hidden_size)), requires_grad=True)  # Adjusted shape\n",
    "        self.w2 = tensor(np.random.normal(size=(hidden_size, output_size)), requires_grad=True)\n",
    "        self.b2 = tensor(np.random.normal(size=(1, output_size)), requires_grad=True)  # Adjusted shape\n",
    "        self.optimizer = SGD([self.w1, self.b1, self.w2, self.b2], lr=lr)\n",
    "\n",
    "    \n",
    "    def tanh(self, tensor):\n",
    "        new_data = np.tanh(tensor.data)\n",
    "        grad_fn = TanhNode(tensor)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, prediction, target):\n",
    "        \n",
    "        sig = 1/(1+np.exp(-prediction.data))\n",
    "        result = -target.data * np.log(sig) - (1-target.data) * np.log(1-sig)\n",
    "        \n",
    "        return Tensor(result, grad_fn=BinaryCrossEntropyLossNode(prediction, target))\n",
    "\n",
    "    \n",
    "    def sigmoid(self, tensor):\n",
    "        new_data = 1 / (1 + np.exp(-tensor.data))\n",
    "        grad_fn = Sigmoid(tensor)\n",
    "        return Tensor(new_data, grad_fn=grad_fn)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        hidden = X @ self.w1 + self.b1\n",
    "        hidden_activation = self.tanh(hidden)\n",
    "        output = hidden_activation @ self.w2 + self.b2\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def train(self, X, Y, epochs=1000):\n",
    "        for epoch in range(epochs):\n",
    "            for row in range(X.shape[0]):\n",
    "                x_row = X[row, :] \n",
    "                y_true = Y[row]    \n",
    "\n",
    "                x_row_tensor = Tensor(x_row[np.newaxis, :])  #\n",
    "                y_true_tensor = Tensor(np.array([[y_true]]))  #\n",
    "\n",
    "                y_pred = self.forward(x_row_tensor)\n",
    "\n",
    "                #BCE loss\n",
    "                loss = self.binary_cross_entropy_loss(y_pred, y_true_tensor)\n",
    "\n",
    "                #backward\n",
    "                loss.backward()\n",
    "\n",
    "                #update\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        correct = 0\n",
    "        total = len(Y_test)\n",
    "        for i in range(total):\n",
    "            x_test_row = X_test[i]\n",
    "            y_test_true = Y_test[i]\n",
    "            x_test_row_tensor = Tensor(x_test_row[np.newaxis, :])\n",
    "            y_test_true_tensor = Tensor(np.array([[y_test_true]]))\n",
    "\n",
    "            y_test_pred = self.forward(x_test_row_tensor).item()\n",
    "            if (y_test_pred >= 0.5 and y_test_true == 1) or (y_test_pred < 0.5 and y_test_true == 0):\n",
    "                correct += 1\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for row in range(X.shape[0]):\n",
    "            x_row = X[row, :]\n",
    "            x_row_tensor = Tensor(x_row[np.newaxis, :])\n",
    "            y_pred = self.forward(x_row_tensor)\n",
    "            predictions.append(y_pred.item())\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee235a-f498-46ec-b737-9693afbed355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.08280642395616747\n",
      "Epoch 10, Loss: 0.06792364654262412\n",
      "Epoch 20, Loss: 0.08064875144371783\n",
      "Epoch 30, Loss: 0.0644046645570351\n",
      "Epoch 40, Loss: 0.059926449917948266\n",
      "Epoch 50, Loss: 0.04087231089806514\n",
      "Epoch 60, Loss: 0.049329726428653325\n",
      "Epoch 70, Loss: 0.054449015583669734\n",
      "Epoch 80, Loss: 0.05584741861078787\n"
     ]
    }
   ],
   "source": [
    "input_size = Xtrain.shape[1]\n",
    "hidden_size = 7\n",
    "output_size = 1\n",
    "\n",
    "#init NN\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size, lr=0.01)\n",
    "\n",
    "#train model\n",
    "model.train(Xtrain, Ytrain, epochs=100)\n",
    "\n",
    "accuracy = model.evaluate(Xtest, Ytest)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3e10e-330c-4c49-80c3-c00425acb4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b39758-f9a3-4b2c-899b-61085234e78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

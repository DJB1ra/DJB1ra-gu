{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49c84375-3b26-4092-b6dc-fd169f2dd269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd545e-94d2-4fbc-8126-b6442292050f",
   "metadata": {
    "tags": []
   },
   "source": [
    "<H2> Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3adf20d0-6ada-427c-aeb6-57dccb1a6c04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n",
      "Classes: ['MEL', 'NV']\n",
      "Number of training images: 6426\n",
      "Number of training subset images: 1285\n",
      "Number of validation images: 1252\n"
     ]
    }
   ],
   "source": [
    "#transform to 224, 224 since VGG expect that size\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Define paths to your train and validation data\n",
    "train_path = \"train\"\n",
    "val_path = \"val\"\n",
    "\n",
    "#define train and valdataset again but with transoformers this time\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "val_dataset = ImageFolder(val_path, transform=transform)\n",
    "\n",
    "#8 batch size to reduce training time\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#20% of training since otherwise it would have taken a day to train\n",
    "subset_size = int(0.2 * len(train_dataset))\n",
    "\n",
    "# Randomly select a subset of indices from the training dataset\n",
    "subset_indices = np.random.choice(len(train_dataset), subset_size, replace=False)\n",
    "\n",
    "# Create a subset of the training dataset using the selected indices\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "# Use DataLoader to load the subset of data in batches\n",
    "train_loader_subset = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Print dataset details\n",
    "print(\"Number of classes:\", len(train_dataset.classes))\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "print(\"Number of training images:\", len(train_dataset))\n",
    "print(\"Number of training subset images:\", len(train_subset))\n",
    "print(\"Number of validation images:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346b786-f856-4bbc-88a7-a53c0e7c1c39",
   "metadata": {},
   "source": [
    "<h2> Some baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da724635-779b-4f5c-9af8-1755e91d25f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83d47974-c009-466f-832c-39c804396488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9741667-15d8-425c-9fc5-cc70ff15e01f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Training): 100%|██████████| 161/161 [00:39<00:00,  4.09batch/s, accuracy=0.735, loss=0.538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 0.5380, Training Accuracy: 73.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 (Validation): 100%|██████████| 157/157 [00:14<00:00, 10.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Validation Loss: 0.4880, Validation Accuracy: 73.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 (Training): 100%|██████████| 161/161 [00:38<00:00,  4.15batch/s, accuracy=0.742, loss=0.539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Training Loss: 0.5386, Training Accuracy: 74.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 (Validation): 100%|██████████| 157/157 [00:13<00:00, 11.59batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Validation Loss: 0.4880, Validation Accuracy: 73.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 (Training): 100%|██████████| 161/161 [00:39<00:00,  4.11batch/s, accuracy=0.742, loss=0.532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Training Loss: 0.5324, Training Accuracy: 74.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 (Validation): 100%|██████████| 157/157 [00:14<00:00, 10.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Validation Loss: 0.4880, Validation Accuracy: 73.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 (Training): 100%|██████████| 161/161 [00:40<00:00,  3.97batch/s, accuracy=0.733, loss=0.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Training Loss: 0.5396, Training Accuracy: 73.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 (Validation): 100%|██████████| 157/157 [00:13<00:00, 11.31batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Validation Loss: 0.4880, Validation Accuracy: 73.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 (Training): 100%|██████████| 161/161 [00:40<00:00,  3.98batch/s, accuracy=0.738, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Training Loss: 0.5331, Training Accuracy: 73.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 (Validation): 100%|██████████| 157/157 [00:13<00:00, 11.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Validation Loss: 0.4880, Validation Accuracy: 73.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(train_loader_subset, desc=f\"Epoch {epoch+1}/{num_epochs} (Training)\", unit=\"batch\") as t:\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            t.set_postfix(loss=running_loss / total, accuracy=correct / total)\n",
    "\n",
    "    # Print training statistics\n",
    "    epoch_loss = running_loss / len(train_subset)\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2%}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Validation)\", unit=\"batch\"):\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels).item() * val_inputs.size(0)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_total += val_labels.size(0)\n",
    "            val_correct += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "    # Print validation statistics\n",
    "    val_epoch_loss = val_loss / len(val_dataset)\n",
    "    val_epoch_acc = val_correct / val_total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f8ef5-52ba-4b6c-8b42-0cfdcd42c217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ea11815-7ad0-496a-8dcb-b5d63b60cf4c",
   "metadata": {},
   "source": [
    "<h2> VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "532fadce-db7b-4f29-9b56-6b143df39a03",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "num_cpu_cores = os.cpu_count()\n",
    "print(num_cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ae3aea-f251-4c7f-adb3-c019873951c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_transforms = transforms.RandomApply([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.15, saturation=0.1, hue=0.1)\n",
    "], p=0.2)  # Apply each transformation with 20% probability\n",
    "\n",
    "# Use the provided vggtransforms for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    random_transforms,               # Apply random transformations for data augmentation\n",
    "    transforms.Resize((224, 224)),   # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  #preprocess images according to\n",
    "    #imagenet numbers\n",
    "])\n",
    "\n",
    "#define train and valdataset again but with complex transformers\n",
    "#need to redefine everything so the new transformers are applied correctly\n",
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "val_dataset = ImageFolder(val_path, transform=transform)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_cpu_cores)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cpu_cores)\n",
    "\n",
    "subset_size = int(0.2 * len(train_dataset))\n",
    "\n",
    "subset_indices = np.random.choice(len(train_dataset), subset_size, replace=False)\n",
    "\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "train_loader_subset = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_cpu_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aa2e9d6-fb80-4899-92c2-d35a8c007fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load in weights from the model\n",
    "weights_id = torchvision.models.VGG16_Weights.IMAGENET1K_V1\n",
    "\n",
    "vggmodel = models.vgg16(weights=weights_id)\n",
    "vggmodel.eval()  # Set the model to evaluation mode\n",
    "\n",
    "for param in resnet_model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Modify the classifier layer\n",
    "num_features = vggmodel.classifier[6].in_features\n",
    "vggmodel.classifier[6] = nn.Linear(num_features, len(train_dataset.classes))\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model = vggmodel.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vggmodel.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4049571d-f6f6-47e5-bd7c-95643f98779f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 804/804 [24:25<00:00,  1.82s/batch, accuracy=0.796, loss=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.3113, Train Accuracy: 79.55%, Val Accuracy: 84.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 804/804 [23:28<00:00,  1.75s/batch, accuracy=0.807, loss=0.457]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 2.2853, Train Accuracy: 80.66%, Val Accuracy: 83.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 804/804 [23:53<00:00,  1.78s/batch, accuracy=0.804, loss=0.474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 2.3708, Train Accuracy: 80.42%, Val Accuracy: 82.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 804/804 [22:53<00:00,  1.71s/batch, accuracy=0.807, loss=0.471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 2.3575, Train Accuracy: 80.72%, Val Accuracy: 84.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 804/804 [24:06<00:00,  1.80s/batch, accuracy=0.809, loss=0.477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 2.3846, Train Accuracy: 80.91%, Val Accuracy: 80.43%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    vgg_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as t:\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = vgg_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            t.set_postfix(loss=running_loss / total_train, accuracy=correct_train / total_train)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    vgg_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = vgg_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    # Print epoch statistics\n",
    "    epoch_loss = running_loss / len(train_subset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.2%}, Val Accuracy: {val_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3a3e1-ee16-4b5e-a10c-42cc501ae511",
   "metadata": {},
   "source": [
    "Epoch [5/5], Loss: 2.3846, Train Accuracy: 80.91%, Val Accuracy: 80.43% all transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6da4b80-54eb-43db-a587-18aab19516ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 157/157 [04:57<00:00,  1.90s/batch, accuracy=0.811, loss=0.395]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3952, Validation Accuracy: 81.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "vggmodel.eval()\n",
    "\n",
    "# Variables to keep track of accuracy and loss\n",
    "val_loss = 0.0\n",
    "val_correct = 0\n",
    "val_total = 0\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "with tqdm(val_loader, desc=\"Validation\", unit=\"batch\") as t:\n",
    "    # Disable gradient computation for validation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation set\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = vggmodel(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Compute loss\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            t.set_postfix(loss=val_loss / val_total, accuracy=val_correct / val_total)\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "avg_val_loss = val_loss / len(val_dataset)\n",
    "val_accuracy = val_correct / val_total\n",
    "\n",
    "# Print validation results\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80f7b6-6cbc-4775-904f-c3c120d43b65",
   "metadata": {},
   "source": [
    "<H2> ResNet Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd1fd227-6df2-490d-b931-3d71a2e20449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "13bf196d-dd50-4687-b49f-ece66bbe0499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define settings\n",
    "use_data_augmentation = True\n",
    "use_batch_norm = True\n",
    "use_weight_decay = True\n",
    "use_subset_training = True\n",
    "batch_size = 8\n",
    "lr = 0.0001\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1cc673c6-dd1a-4cd7-8a3b-a00cc40aa936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "random_transforms = transforms.RandomApply([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.15, saturation=0.1, hue=0.1)\n",
    "], p=0.2) if use_data_augmentation else None\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    random_transforms,\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd727c11-6d00-40ba-8366-813a741db913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(train_path, transform=transform)\n",
    "val_dataset = ImageFolder(val_path, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_cpu_cores)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_cpu_cores)\n",
    "\n",
    "# Subset training data if enabled\n",
    "if use_subset_training:\n",
    "    subset_size = int(0.2 * len(train_dataset))\n",
    "    subset_indices = np.random.choice(len(train_dataset), subset_size, replace=False)\n",
    "    train_subset = Subset(train_dataset, subset_indices)\n",
    "    train_loader_subset = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_cpu_cores)\n",
    "else:\n",
    "    train_loader_subset = train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9718073e-342a-4fa3-9dd4-b3bee6f9c625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet model\n",
    "resnet_model = models.resnet18(weights=True)\n",
    "\n",
    "# Freeze the parameters of the model\n",
    "if not use_batch_norm:\n",
    "   # Unfreeze last layer\n",
    "    for param in resnet_model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Add batch normalization if enabled\n",
    "if use_batch_norm:\n",
    "    for param in resnet_model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    for module in resnet_model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            module.add_module('batch_norm', nn.BatchNorm2d(module.out_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab1a55e3-2013-433f-b2b6-d68d52add96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the classifier layer\n",
    "num_features = resnet_model.fc.in_features\n",
    "resnet_model.fc = nn.Linear(num_features, len(train_dataset.classes))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_params = {'lr': lr}\n",
    "if use_weight_decay:\n",
    "    optimizer_params['weight_decay'] = 0.0001\n",
    "optimizer = optim.Adam(resnet_model.parameters(), **optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8ac7e-65e1-4afb-90bb-7724002a519f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 804/804 [12:35<00:00,  1.06batch/s, accuracy=0.811, loss=0.414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.0690, Train Accuracy: 81.08%, Val Accuracy: 85.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 804/804 [12:03<00:00,  1.11batch/s, accuracy=0.852, loss=0.337]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 1.6858, Train Accuracy: 85.20%, Val Accuracy: 84.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 804/804 [17:40<00:00,  1.32s/batch, accuracy=0.874, loss=0.292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 1.4610, Train Accuracy: 87.41%, Val Accuracy: 88.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 804/804 [18:32<00:00,  1.38s/batch, accuracy=0.899, loss=0.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 1.2588, Train Accuracy: 89.90%, Val Accuracy: 89.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:  27%|██▋       | 215/804 [05:18<14:00,  1.43s/batch, accuracy=0.922, loss=0.21] "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    resnet_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as t:\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = resnet_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update tqdm progress bar\n",
    "            t.set_postfix(loss=running_loss / total_train, accuracy=correct_train / total_train)\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    resnet_model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = resnet_model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    # Print epoch statistics\n",
    "    epoch_loss = running_loss / len(train_subset) if use_subset_training else running_loss / len(train_dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Train Accuracy: {train_accuracy:.2%}, Val Accuracy: {val_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3226257-94f6-4633-82d1-4ae4c761dd87",
   "metadata": {},
   "source": [
    "Epoch [5/5], Loss: 0.4807, Train Accuracy: 77.04%, Val Accuracy: 78.51% No transformers or pre-processing TRAIN SUBSET\n",
    "\n",
    "Epoch [5/5], Loss: 0.4041, Train Accuracy: 79.77%, Val Accuracy: 83.15% transformers but no mean/std transform TRAIN SUBSET\n",
    "\n",
    "Epoch [5/5], Loss: 0.3981, Train Accuracy: 81.32%, Val Accuracy: 81.39% Transformers with Mean/std transform TRAIN SUBSET\n",
    "\n",
    "Epoch [5/5], Loss: 0.4406, Train Accuracy: 79.38%, Val Accuracy: 84.03% Transformers with Mean/std transform AND weight_decay in optimizer TRAIN SUBSET\n",
    "\n",
    "Epoch [5/5], Loss: 2.1635, Train Accuracy: 79.46%, Val Accuracy: 82.99% All transformers and whole training set \"higher\" values on image augmentation\n",
    "\n",
    "Epoch [5/5], Loss: 2.0784, Train Accuracy: 81.03%, Val Accuracy: 82.83%\n",
    "Lowered image tranformations, lots of the misclassified images were very altered. Also lower % of pictures tranformed to 20%. WHOLE TRAINING SET \n",
    "\n",
    "Epoch [5/5], Loss: 2.0262, Train Accuracy: 81.61%, Val Accuracy: 82.11% Resized to 128, 128, batch_size 16. All transformers left as they were. Lower values on the image alteration. WHOLE TRAINING SET\n",
    "\n",
    "Epoch [5/5], Loss: 1.9423, Train Accuracy: 82.56%, Val Accuracy: 84.19% Resize 224, 224, batch size 16, all transformers, added Batch_normalization after resnet_model is defined. WHOLE TRAINING SET\n",
    "\n",
    "Epoch [5/5], Loss: 1.9692, Train Accuracy: 81.23%, Val Accuracy: 83.07%\n",
    "Same as above but added a fully connected layer at the end. WHOLE TRAINING SET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fef56-59b3-4b9a-b7a9-526d77b9b6fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a directory to save misclassified images\n",
    "output_dir = 'misclassified_images'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Lists to store misclassified image paths, labels, and predictions\n",
    "misclassified_image_paths = []\n",
    "misclassified_labels = []\n",
    "misclassified_predictions = []\n",
    "\n",
    "# Validation phase\n",
    "resnet_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = resnet_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_val += labels.size(0)\n",
    "        correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Find misclassified images\n",
    "        misclassified_mask = predicted != labels\n",
    "        misclassified_images = inputs[misclassified_mask]\n",
    "        misclassified_labels.extend(labels[misclassified_mask].tolist())\n",
    "        misclassified_predictions.extend(predicted[misclassified_mask].tolist())\n",
    "        \n",
    "        # Save misclassified images\n",
    "        for i, image in enumerate(misclassified_images):\n",
    "            image_path = os.path.join(output_dir, f'misclassified_{len(misclassified_image_paths) + 1}.png')\n",
    "            misclassified_image_paths.append(image_path)\n",
    "            \n",
    "            # Normalize image pixel values to the range [0, 1]\n",
    "            image = image.cpu().numpy().transpose((1, 2, 0))\n",
    "            image = (image - image.min()) / (image.max() - image.min())\n",
    "            \n",
    "            plt.imsave(image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3c473-0be3-4a24-9861-6a540f6c55be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
